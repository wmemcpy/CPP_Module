Merge insert sort (d√©couvert par Ford et Johnson, probablement nomm√© par D. Knuth) est un algo qui cherche a minimiser le nombre de comparaisons effectu√©es pour trier une liste. Cet algo utilise la recherche dichotomique (binary search en anglais). C'est parfois appel√© la recherche par dictionnaire (je compare mon √©l√©ment √† celui du milieu de la liste, pour diviser la taille de mon champ de recherche par deux, et je recommence). Cette technique permet d'ins√©rer un √©l√©ment dans une liste de taille n en log2(taille liste) comparaisons.

**L'algo:**

En entr√©e: une liste non vide d'√©l√©ments.
En sortie: une liste tri√©e de ces √©l√©ments.
√©tape 1: Merge. Si il y n'y a qu'un √©l√©ment dans la liste, la retourner, sinon, appairer les √©l√©ments et trier les deux √©l√©ments de chaque paire. Faire une liste de ces paires.
√©tape 2: R√©cursion. Appeler cet algorithme sur cette liste de paire.
√©tape 3: Insert. Extraire de la liste de paire (qui a donc √©t√© tri√©e lors de l'appel r√©cursif √† l'√©tape 2) une liste tri√©e des plus grands √©l√©ments de chaque paire. Ins√©rer le plus petit √©l√©ment  de chaque paire dans la liste tri√©e en suivant une progression de Jacobsthal. Retourner la liste tri√©e.

N.B. attention au terme "√©l√©ment": lors de l'appel r√©cursif, on appelle notre fonction sur une liste de paire d'√©l√©ments. Pour l'instance de la fonction qui recevra cette liste de paire, cette liste est une liste d'√©l√©ments.

**Les explications**

**√âtape 1**: je fais des paires d'√©l√©ments (e.g. en doublant la longueur de mes it√©rateurs), et je trie les deux √©l√©ments entre eux. A la fin de l‚Äô√©tape 1, j'ai une liste de paires d'√©l√©ments (dans l'explication de l‚Äô√©tape 3, cette paire sera not√©e P(a, b) avec a le petit √©l√©ment, et b le grand); si on me donne le plus grand √©l√©ment d'une paire, il faut que je puisse retourner le plus petit imm√©diatement (c'est √† dire sans faire de comparaisons, de parcours de liste (et donc de comparaisons...)... l‚Äôid√©e, c'est de faire le moins de comparaisons possible).

**Etape 2**: je suis en train de coder un algo de tri. Quand j'aurais fini et que je l'aurais compil√©, j'aurais une fonction qui, quand on lui passe une liste d'√©l√©ments, la trie. Il se trouve que dans cet algorithme, j'ai besoin de trier une liste deux fois plus petite que celle re√ßue en entr√©e: j'utilise cette fonction. Pourquoi √ßa marche: ¬†il y a une remarque √† se faire: une liste de taille 1 est tri√©e. A force de diviser la taille de notre liste par deux (au fil des appels r√©cursifs successifs), on va bien arriver √† une liste de taille 1, et la, la fonction pourra me renvoyer une liste tri√©e. De cette liste tri√©e, je vais donc pouvoir produire une liste tri√©e de taille deux et la renvoyer. L'appel r√©cursif du dessus va recevoir une liste de taille deux tri√©e et pouvoir produire une liste tri√©e de taille 4...
J'ai maintenant une liste de paires d'√©l√©ments tri√©e suivant le plus grand √©l√©ment de chaque paire.

Une premi√®re **observation** ici: le petit √©l√©ment de la paire la plus petite (donc dont le plus grand √©l√©ment est plus petit que tous les autres plus grands √©l√©ments) est plus petit que tous les √©l√©ments de cette liste (la liste des plus grands √©l√©ments de chaque paire... ): je vais donc pouvoir l'ins√©rer √† l'√©tape trois sans avoir √† effectuer aucune comparaison... (Et c'est l√† qu'il faut pouvoir acc√©der √† cet √©l√©ment depuis l'√©l√©ment avec lequel il a √©t√© appair√© sans avoir √† faire de comparaisons).

Une deuxi√®me **observation**: apr√®s une insertion, la liste sera toujours tri√©e.
	
Une **digression**: vu que je vais devoir ins√©rer des √©l√©ments dans une liste tri√©e, je vais pouvoir faire des recherches par dichotomie. C'est donc mieux si j'ai 2^n-1 √©l√©ments dans ma liste (pour un certain n... encore une histoire de log).
Plus concr√®tement: on va commencer par regarder combien de comparaisons je dois faire pour ins√©rer un √©l√©ment dans des listes de diff√©rentes tailles; et on va essayer de remarquer que le nombre de comparaisons n√©cessaires augmente de un quand on passe d'une liste de taille 2^n-1 √† une liste de taille 2^n. Si je veux ins√©rer un √©l√©ment dans une liste de:
- 0 (= 2^0-1) √©l√©ments: j'ai juste √† l'ins√©rer: 0 comparaisons
- 1 (= 2^1-1 = 2^0) √©l√©ment: j'ai une comparaison √† faire et √† ins√©rer: 1 comparaison
- 2 √©l√©ments: une (si j'ai de la chance: je devais ins√©rer 1 dans la liste (3,5), j'ai choisi de commencer par comparer avec 3, et j'ai pu l'ins√©rer direct) ou deux comparaisons (si j'ai eu moins de chance: je devais ins√©rer 4); on se base sur le pire cas: 2 comparaisons
- 3 (= 2^2-1) √©l√©ments: une comparaison avec celui de milieu qui va me dire de quel c√¥t√© me diriger, une liste de un de chaque cote(et j'ai d√©j√† not√© que pour une liste de un, il me fallait une comparaison): 1 + 1 = 2 comparaisons, comme pour une liste de 2 √©l√©ments.
- 4 √©l√©ments: un peu comme pour les listes de deux √©l√©ments, √ßa va d√©pendre de si j'ai de la chance ou pas; deux ou trois comparaison: 3 comparaisons
- 5 √©l√©ments: une comparaison avec celui du milieu, une liste de deux de chaque c√¥t√©: 3 comparaisons aussi
- 6 √©l√©ments: je te la fais courte: encore 3 comparaisons
- 7 (= 2^2-1) √©l√©ments: une comparaison avec celui du milieu, une liste de trois de chaque c√¥t√©: toujours 3 comparaisons et √† 8 √©l√©ments, on passe √† 4 comparaisons; √† 16 √† 5... Et donc avec une recherche par dichotomie, rechercher dans une liste de 8 ou de 15 √©l√©ments, √ßa prend le m√™me temps.

**√âtape 3**: En reprenant la notation des paire P(a, b), j'ins√®re donc le petit √©l√©ment "a1" de la paire la plus petite P1(a1, b1) (P1 parce que c'est la plus petite, ensuite, P2, P3...) sans faire de comparaisons. Conceptuellement, j'ai maintenant une liste a1, b1, P2, P3... (Conceptuellement parce que je suis en train de faire une liste ou je m√©lange des √©l√©ments (a1, b1), et des paires d'√©l√©ments (P2...): mon compilateur me ferait la gueule)
J'ai commenc√© par P1.
Je pourrais ins√©rer le petit √©l√©ment a2 de P2(a2, b2), mais je l'ins√©rerai ¬†dans une liste de taille 2: 2 comparaisons, et ensuite, je devrais ins√©rer le petit √©l√©ment a3 de P3 dans une liste de 4 √©l√©ments (a1, b1, a2 et b2): donc 3 comparaisons (le nombre de comparaisons pour les listes de taille n est juste au dessus), pour un total de 5 comparaisons.
¬†Alors que si j'ins√®re le petit √©l√©ment a3 de P3(a3, b3) dans la liste [a1, b1, P2] (techniquement, on l'ins√©rera dans [a1, b1, b2]), puis que j'ins√®re a2 (soit dans la liste [a1, b1], soit dans une liste compos√©e de a1, b1 et a3), donc deux insertions dans des listes de taille 3, √ßa fait chuter le total √† 4 comparaisons.
Donc les suivants, c'est P3 puis P2.
Maintenant, j'ai dans ma liste a1, b1, a2, b2, a3 et b3 tri√©s: 6 √©l√©ments. Exactement comme la fois pr√©c√©dente, si j'essaye d'ins√©rer a4 dans une liste de 6, j'ins√©rerai a5 dans une liste de 8: donc 3 + 4 = 7 comparaisons, alors que si j‚Äôins√®re a5 puis a4, les deux seront dans des listes d'au plus 7, pour un total de 6 comparaisons.
Et les suivants: P5, P4.Next one.
Maintenant, dans ma liste, j'ai a1, b1, a2, b2, a3, b3, a4, b4 a5 et b5: 10 √©l√©ments. Le prochain 2^n-1, c'est 15: je peux donc aller jusqu‚Äô√† P11 (j'ai 10 √©l√©ments, dans ma liste, et j'en veux 15: il m'en manque 5, je suis a P5, il me faut donc ceux de P6√† P10, il faut donc que je commence par P11), et je ne ferais que des insertions dans des listes de 15 √©l√©ments (donc max 4 comparaisons !) .
Puis P11, P10, P9, P8, P7, P6.
un petit dernier:
Quand j'aurais ins√©r√© P11, P10, P9, P8, P7 et P6, j'aurais 22 √©l√©ments dans ma liste, le prochain 2^n-1, √ßa sera 31; 31 - 22, il en manque 9, je suis √† p11, et je dois prendre le suivant: 11 + 9 + 1 = 21
Et P21...
A chaque √©tape,on a fissionn√© des paires pour les ins√©rer dans une liste de taille 2^(n-1)-1, et on cherche de combien il faut qu'on avance pour ins√©rer nos √©l√©ments dans une liste de taille 2^n-1; on commence √† voir un motif √©merger:
A la prochaine √©tape, il nous manquera ce qu'il nous manque plus deux fois ce qu'il nous manquait √† l'√©tape pr√©c√©dente...
Donc j‚Äôins√®re mes √©l√©ments suivant la suite de Jacobsthal'  j(n+1) = jn + 2* j(n-1)  (j0 = 1, J1 = 3) pour minimiser le nombre de comparaisons (sur Jacobsthal, l'explication de Knuth est plus rigoureuse, m√™me si il va un peu vite dans ses calculs: quand il divise par 3, il "oublie" de multiplier par (2+1) et d'√©tendre le tout... Mais si on rajoute ces √©tapes, √ßa se lit bien; le lien est juste en dessous)

Et j'ai une liste d'√©l√©ments tri√©e √† renvoyer.

Des liens pour aller plus loin:
- Pour impl√©menter Jacobsthal et apprendre des trucs sur les templates:
https://medium.com/zerone-magazine/templates-and-compile-time-execution-c22234a6cd66
- Pour y comprendre quelque chose √† ces histoires de log (ce mec est un g√©nie de p√©dagogie : pour tous ceux qui se disent qu'ils veulent rattraper leur niveau en maths, je ne saurais que trop conseiller sa cha√Æne Youtube):
https://www.youtube.com/live/cEvgcoyZvB4?feature=share
- Une impl√©mentation en cpp pas 98:
https://codereview.stackexchange.com/questions/116367/ford-johnson-merge-insertion-sort
- Et Knuth (p. 185):
https://doc.lagout.org/science/0_Computer%20Science/2_Algorithms/The%20Art%20of%20Computer%20Programming%20%28vol.%203_%20Sorting%20and%20Searching%29%20%282nd%20ed.%29%20%5BKnuth%201998-05-04%5D.pdf


------------------------


Ford et Johnson ont propos√© un algo de tri qui minimise le nombre de comparaisons. Ce n'est pas un algo fait pour √™tre ex√©cut√© sur un ordinateur (m√™me si c'est un bon exercice de l'impl√©menter; mais une comparaison, pour nos processeurs actuels, c'est une op√©ration √©l√©mentaire). Par contre, c'est un algo utile si on nous demande de programer le logiciel qui doit orchestrer les tests pour classer les produits de notre client. Surtout si ces tests sont longs et chers (contexte industriel oblige). L√†, notre expertise peut lui faire √©conomiser pas mal de temps et d'argent, et donc se n√©gocie bien (ou nos fourmis exploiteront de meilleures ressources plus vite...). Et c'est dans ces contextes qu'il il prend son int√©r√™t.PS: Une note amusante: merge sort est aussi un algo de tri r√©cursif. Insert sort, lui, est consid√©r√© comme un algorithme it√©ratif (bien qu'on puisse l'impl√©menter en r√©cursif... Et qu'on puisse coder merge sort en iteratif (en bottom up...). C'est subtil !).

Apparemment, certains se basent encore sur l'explication de punkchameleon (sur GitHub), qui n'a pas compris la recursion (le reste de ses explications est bon; un mec de 42 Japan a ouvert un ticket sur le sujet), et qui passent donc √† c√¥t√© d'un concept important: leur fourmies seront moins performantes (ou je vais pouvoir montrer √† leur client que je fais mieux que leur expert et r√©cup√©rer le contrat), c'est dommage. On s'est inscrit √† 42, pas en formation p√¥le emploi, 3 mois pour apprendre java: c'est aussi pour √™tre challeng√©. Ca me donne l'opportunit√© (le plaisir m√™me...üßÇ) de revenir deux secondes sur les algorithmes, la r√©cursivit√©... Et les algos recursifs !

D'abord, de quoi on parle:

Sur wikip√©dia, un algorithme c'est :
"une suite finie et non ambigu√´ d'instructions et d‚Äôop√©rations permettant de r√©soudre une classe de probl√®mes."
Notons bien la non ambigu√Øt√©. Quand on parle avec quelqu'un qui a compris l'algo, on sait qu'on parle de la m√™me chose. Si on n'a pas compris un algo pareil, c'est qu'au moins une des deux personnes ne l'a pas compris (ce qui ne veut pas dire qu'il ne peut pas y avoir de confusion: si, je me mettai a utiliser la reverse polish notation dans mes explications, tant que vous n'aurez pas remarqu√© que j'utilise pas le syst√®me standard, on va pas forc√©ment avoir l'impression de parler de la m√™me chose. Ou si j'impl√©mentais mon algo pour trier par ordre d√©croissant : tout serait invers√© et √ßa va demander une petite gymnastique pour se rendre compte que, in fine, c'est la m√™me proc√©dure). Apr√®s, les algos peuvent √™tre organis√©s diff√©remment dans leurs impl√©mentations, les explications peuvent √™tre plus ou moins pertinentes (et r√©sonner plus ou moins avec nous, et notre compr√©hension des choses). Je ne dis pas qu'il n'y a qu'une mani√®re de voir les choses ni que la mienne est bonne. Mais qu'il y a des choses qui sont quand m√™me assez clairement d√©finies. Apr√®s, pour ceux qui pensent que la terre est plate, qu'il y avait plus de monde √† l'investiture de Trump qu'√† celle d'Obama... Bref, qu'il suffit de dire une chose suffisamment fort pour en faire une r√©alit√©, √ßa va √™tre compliqu√©.

Tant qu'√† tout d√©finir, la classe de probl√®me qui nous int√©resse, c'est le tri de listes. De n'importe quelle liste.

Toujours sur wikip√©dia, la r√©cursivit√© est d√©finie comme:
"une d√©marche qui fait r√©f√©rence √† l'objet m√™me de la d√©marche √† un moment du processus (...). En informatique (...), une fonction ou plus g√©n√©ralement un algorithme peut contenir un ou des appels √† lui-m√™me, auquel cas il est dit r√©cursif."

Donc, punkchameleon (son code fonctionne: √ßa trie), dans ses explications:
"Step 4, The next step in FJMI is to recursively sort all the pairs by their largest element. In this implementation, we use ‚Äòsort_by_larger_value‚Äô (...), a modified insertion sort."
√âtape 4, la prochaine √©tape de FJMI est de trier recursivement la liste de paires suivant leurres plus grands elements.
Le probl√®me, c'est que pour trier recursivement, il faut comprendre le principe de r√©cursivit√©...

Et moi, dans mon explication pr√©c√©dente : une fonction n'est pas r√©cursive parce qu'elle appelle une fonction r√©cursive, une fonction est r√©cursive car elle s'appelle elle m√™me (c'est la m√™me erreur, mais l√† o√π l'on faisait appel a merge sort, il fait appel a insertion sort, aka insert sort, qu'il impl√©mente en r√©cursif...).

Maintenant que j'en ai fini avec les d√©finitions et les citations, place aux explications:

Oublions qu'on est en train de coder merge insert sort, et disons qu'on est en train de d√©crire une proc√©dure pour trier une liste : √† une certaine √©tape de la proc√©dure, je dois trier une autre liste; l'utilisation de n'importe quel algo de tri rendra la proc√©dure fonctionelle.

Si l'algo de tri qu'on utilise est insert sort (respectivement merge sort), notre algo de tri, c'est insert sort, et notre proc√©dure est une d√©coration*.
Par contre, si, √† cette √©tape la, notre proc√©dure s'appelle elle m√™me (et que ca fonctionne), alors notre proc√©dure est un algo de tri.

*Je dis d√©coration, mais ici, on est sur du Rococo, baroque tardif (c'est tr√®s compliqu√© et on y comprend pas grand chose: la bonne nouvelle, c'est que c'√©tait la partie dure de l'algo). Une proc√©dure plus minimaliste pourrait √™tre:

"Ma proc√©dure pour trier une liste
√âtape 1: ajouter 1 √† tous les √©l√©ments de la liste.
√âtape 2: appeler un algo de tri sur cette nouvelle liste.
√âtape 3: retirer 1 √† tous les √©l√©ments de la liste tri√©e.
Retourner la liste tri√©e."

Cette proc√©dure fonctionne (√ßa tri). Mais si a l'√©tape 2, j'essaye d'appeler recursivement ma proc√©dure plut√¥t que d'appeler un algo de tri, il ne va pas se passer grand chose  (√ßa va overflow √† la limite): cette proc√©dure n'est donc pas un algo de tri (pourtant, elle trie une liste... Mais l'algo de tri, c'est le truc que j'ai appel√© √† l'√©tape 2: mes √©tapes 1 et 3 sont d√©coratives. Je pourrais les suprimer, ca fonctionnerait tout aussi bien).

Similairement, le code de punkchameleon est une proc√©dure de tri tr√®s compliqu√©e, qui comme merge-insert sort, appaire les √©l√©ments pour les ins√©rer ensuite suivant la progression de jacobsthal, mais fait, entre les deux √©tapes, appel √† insert sort: l'algo de tri, c'est insert sort. Le reste, c'est de la d√©co. M√™me si cette d√©co, c'est la proc√©dure de FJ... C'est ballot: sur la moiti√© de notre liste, on n'optimise pas le nombre de comparaisons. On aurait donc pu trier cette liste en en effectuant moins.

Les codes de Morwenn, decidelyso, codeoptimist... C'est des merge-insert sorts. L'algo de tri qu'ils appellent, c'est la fonction qu'ils sont en train de coder. Et √ßa fait toute la diff√©rence.

Tout √ßa pour dire que merge-insert sort est un algo de tri, pas une proc√©dure compliqu√©e qui d√©core un algo de tri.

La truc qui fait de Ford Johnson un algo de tri, c'est qu'on peut appeler recursivement la "proc√©dure" qu'ils d√©crivent, et que √ßa va fonctionner.
Pourquoi √ßa va fonctionner: parce que √ßa trie. Et parce qu'il y a un crit√®re d'arr√™t. Un peu les deux en m√™me temps (d'o√π la num√©rotation wtf).

b) Si j'ai compris pourquoi cette proc√©dure, quand on lui donne une liste, peu importe sa taille, arrive √† la trier (m√™me si pour moi, ce qui se passe lors de cette √©tape r√©cursive est mysterieux: je sais juste que j'ai acc√®s √† une boite noire qui peut trier ma liste de paires (soit, une liste deux fois plus petite que celle que j'ai recue en entr√©e). Peu importe comment. Ce qui est important, c'est que je sois persuad√© que cette procedure, si elle a acc√®s a cette boite noire, est capable de trier n'importe quelle liste)... Pourquoi ne pas faire appel √† cette proc√©dure m√™me, en lieu et place de ma boite noire ?

a) Maintenant, quelle que soit la taille de la liste que je re√ßois initialement, je ne vais pouvoir la couper en deux qu'un certain nombre de fois avant de me retrouver avec une liste √† un seul √©l√©ment, qui, par nature, est tri√©e ( (approximativement) log2(taille liste) fois). Autrement dit, il va y avoir log2(taille liste) appels recursifs √† ma fonction. Et √† partir de l√†, je vais pouvoir "remonter" ma cha√Æne d'appels recursifs successifs.

Mes explications s√ªr le pourquoi √ßa trie et le comment √ßa optimise les comparaisons sont dans le pav√© pr√©c√©dent.

Pour ceux qui pr√©f√®rent la pratique: on va d√©rouler l'algo sur l'instance [7, 8, 5, 9, 11, 0, 2, 1, 10, 3, 4, 6] (avec des bullets pour noter la profondeur dans notre recursion: on est au premier niveau, une bullet)

. Apairrer: [(7, 8), (5, 9), (11, 0), (2, 1), (10, 3), (4, 6)]
. Comparer\*: [8, 9, 11, 2, 10, 6] et... R√©cursion (on va trier cette nouvelle liste de la m√™me mani√®re que la premi√®re, donc une bullet en plus, mais la r√©cursion, c'est la phase 2 de l'algo: on reprendra pour la phase 3 quand l'appel r√©cursif suivant me renverra cette liste tri√©e):
.. Apairrer: [(8, 9), (11, 2), (10, 6)]
.. Comparer\*\*: [9, 11, 10] et... R√©cursion:
... Apairrer: [(9 , 11)] je peux pas faire de paire avec le 10, je le garde pour plus tard.
... Comparer\*\*\*: [11] et... R√©cursion:
.... Crit√®re d'arr√™t atteint (liste de taille 1): je retourne la liste [11]... Et je commence √† "remonter" ma cha√Æne d'appels (je repasse √† 3 bullets).
... Ins√©rer: l'√©l√©ment appair√© √† 11, le 9\*\*, j'ai la liste [9, 11]. J'ins√®re le 10 que je m'√©tais gard√© pour plus tard et je retourne la liste [9, 10, 11]
.. Ins√©rer: d'abord l'√©l√©ment appair√© au plus petit √©l√©ment, 9, ici c'est 8\*, j'ai la liste [8, 9]. Ensuite, j'ins√®re l'√©l√©ment apairr√© √† 11, ici le 2 dans la liste [8, 9, 10] (en fait, dans la liste [8, 9, (10, 6)]...) , puis l'√©l√©ment apairr√© √† 10 (le 6) dans la liste [2, 8, 9]. Je retourne la liste [2, 6, 8, 9, 10, 11]
. Ins√©rer: d'abord l'√©l√©ment appair√© √† 2, puis celui appair√© √†  8, suivi de celui appair√© √† 6, ensuite celui appair√© √†  10, suivi de celui √† 9, puis √† 11 (suivant Jacobsthal...) et j'ai la liste [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11] √† renvoyer.

Ma liste est tri√©e.

\* Attention, abus de notation incoming: plut√¥t que d'utiliser une paire, j'ai utiliser le plus grand √©l√©ment de la paire. Mais c'est bien d'une paire qu'il s'agit : 8 represente la paire (7, 8); 9 c'est (5, 9)...
\*\* J'ai not√© 9, mais en fait, 9 c'est (8, 9)... Mais (8, 9) de l'√©tape pr√©c√©dente: donc 9 c'est  ((7,8), (5,9)) (abus de notation r√©cursif !)
\*\*\* Et 11 ici, c'est (9, 11)... Soit ((8,9), (2,11))... Ou (((7,8),(5,9)), ((1,2),(0,11)))

Et si je repr√©sente √ßa sous forme d'arbre binaire, √ßa explique pourquoi on en voit sur la page de wikip√©dia sur la r√©cursivit√©.

https://previews.123rf.com/images/popaukropa/popaukropa1501/popaukropa150100022/35722304-r%C3%A9cursivit%C3%A9-les-mains-tenant-un-morceau-de-papier-r%C3%A9p%C3%A9tition-du-motif-illustration.jpg

En parlant de repr√©sentation qui font comprendre des trucs: si on devait repr√©senter la r√©cursivit√©, on pourrait voir les choses comme ca.
(Et si cette image est r√©cursive, l'image qui s'affiche sur l'√©cran sur lequel je lis ce texte ne l'est pas. M√™me si il y a une image r√©cursive dedans. Si je la mets en plein ecran, alors l√† oui, l'image affich√©e par mon √©cran serait r√©cursive (ou alors en s'embetant a inclure tout ce qui s'affiche actuellement sur mon ecran dans notre image). Pour les fonctions, c'est pareil: c'est pour √ßa que je me suis auto-cit√© au d√©but).

Pour rentrer √† 42, on a d√ª piloter une fus√©e (et faire un test de m√©moire si je me souviens bien ^^). Fallait aller chercher des √©toiles. Le pilotage de la fus√©e, c'√©tait que du r√©cursif, et il y avait des niveaux assez compliqu√©s. Intrinsequement, c'est un concept que vous ma√Ætrisez. On est certes dans un environnement moins ludique, mais, pour peu de se pencher dessus, √ßa peut √™tre assez fun ces sujets. Faut pas se laisser impressionner par l'√©tiquette info th√©orique/math√©matiques.

J'esp√®re que √ßa dissipe les derni√®res ambigu√Øt√©s et incompr√©hensions qu'il pouvait y avoir. Et parce qu'en fait, la recursion, c'est une √©tape vers des concepts plus avanc√©s (et parce que parfois, √™tre expos√© √† ces concepts plus avanc√©s, √ßa motive la compr√©hension des trucs interm√©diaires)...

Laissez moi vous pr√©senter l'impredicativit√© (l'auto-reference... Ou la r√©cursivit√© sous hallucinog√®nes), parce que, toujours selon wikip√©dia,
 "l'aphorisme suivant : ¬´ Pour comprendre le principe de r√©cursivit√©, il faut d'abord comprendre le principe de r√©cursivit√© ¬ª, est impr√©dicatif".
Et qu'il ne faut pas les confondre.
(C'est cryptique comme exemple, alors qu'en vrai, c'est un sujet passionnant, que c'est la base du projet drquinn... Et, je crois, un concept n√©cessaire √† la g√©n√©ration d'ia g√©n√©rales. Et comme un dessin vaut mille mots, je laisse la conclusion √† un g√©ant sur la question).

https://moa.byu.edu/m-c-eschers-drawing-hands

PS: Une note amusante: merge sort est aussi un algo de tri r√©cursif. Insert sort, lui, est consid√©r√© comme un algorithme it√©ratif (bien qu'on puisse l'impl√©menter en r√©cursif... Et qu'on puisse coder merge sort en iteratif (en bottom up...). C'est subtil !).